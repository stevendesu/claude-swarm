# Autonomous Agent Swarm

A system where autonomous AI agents collaborate through a shared ticketing system, running in isolated Docker containers with their own repository clones. Humans interact through a CLI and web dashboard.

## Directory Layout

```
ai-project-manager/              # This repo (the swarm toolkit)
  agent/
    Dockerfile                    # Agent container image definition
    entrypoint.sh                 # Container startup: validates env, clones repo, starts loop
    agent-loop.sh                 # Main autonomous loop: claim tickets, run claude, git workflow
    check-alive.sh                # Cron script: detects stalled agents, triggers restart
  ticket/
    ticket.py                     # SQLite-backed CLI for task management (Python, stdlib only)
    test_ticket.py                # Test suite (run with: python -m pytest ticket/)
    migrations/                   # SQL migration scripts (001_*.sql, 002_*.sql, etc.)
  monitor/
    Dockerfile                    # Web dashboard container
    server.py                     # Dashboard backend (Python, stdlib only)
    static/index.html             # Self-contained SPA (vanilla HTML/CSS/JS, no frameworks)
  swarm/
    swarm.py                      # The `swarm` CLI — init, start, stop, scale, regenerate
    interview-mcp.py              # MCP server for Phase 2 interview (requires mcp library)
  docker-compose.yml              # LOCAL TEST COPY ONLY — NOT used by deployed swarms
```

A target project after `swarm init`:
```
target-project/
  .swarm/
    agent/                        # Copied from ai-project-manager/agent/
      Dockerfile, entrypoint.sh, agent-loop.sh, check-alive.sh
    ticket/                       # Copied from ai-project-manager/ticket/
      ticket.py, migrations/
    monitor/                      # Copied from ai-project-manager/monitor/
      Dockerfile, server.py, static/
    tickets/
      tickets.db                  # SQLite database (shared volume, WAL mode)
    repo.git/                     # Bare git repo (shared between agents)
    config.json                   # Swarm settings
  docker-compose.yml              # GENERATED by swarm.py — do not edit manually
  CLAUDE.md                       # Agent operating manual (generated by swarm init)
  PROJECT.md                      # Business context (produced by Phase 2 interview)
```

## Critical Gotchas

### 1. Source files vs. deployed files

There are **two copies** of agent files:

- **Source files** live in this repo (`agent/`, `ticket/`, `monitor/`)
- **Deployed files** are copied into a target project's `.swarm/` directory by `swarm init` or `swarm regenerate`

Editing files in `agent/`, `ticket/`, or `monitor/` does **not** affect any running or deployed swarm. Changes only take effect when `swarm regenerate` is run in the target project.

### 2. docker-compose.yml is generated, not static

The `docker-compose.yml` at the repo root is a **local test copy** and is NOT used by the swarm system. Deployed swarms use a `docker-compose.yml` generated by `generate_docker_compose()` in `swarm/swarm.py`.

- To change docker-compose behavior: edit `generate_docker_compose()` in `swarm/swarm.py`
- Do NOT edit `docker-compose.yml` directly expecting it to affect deployed swarms
- After changing `generate_docker_compose()`, run `swarm regenerate` in the target project

### 3. Config is `config.json`, not `config.yml`

The config file lives at `.swarm/config.json` in target projects. Default values:

```json
{
  "agents": 3,
  "ntfy_topic": "",
  "allowed_tools": "Bash,Read,Write,Edit,Glob,Grep",
  "max_turns": 50,
  "monitor_port": 3000
}
```

### 4. OAuth and authentication

- Claude Code stores credentials in the **macOS Keychain** (service: `Claude Code-credentials`), NOT in `~/.claude/`
- `swarm start` extracts the OAuth token via `security find-generic-password -s "Claude Code-credentials" -w` and passes it as `CLAUDE_CODE_OAUTH_TOKEN` env var
- **`ANTHROPIC_API_KEY` does NOT work** with claude.ai OAuth subscriptions (Teams/Enterprise/Max). These are completely separate auth systems with different billing
- If `CLAUDE_CODE_OAUTH_TOKEN` is already set in the shell environment, Keychain extraction is skipped
- Access tokens are short-lived (hours). Token refresh inside containers is not yet implemented — restart the swarm (`swarm stop && swarm start`) to get a fresh token

### 5. The `~/.claude` directory

- Does NOT contain OAuth tokens (those are in the Keychain)
- Contains: `settings.json`, project settings, feature flags, debug logs, history
- Mounted read-only at `/host-claude-config` inside containers (NOT at `~/.claude`) to avoid conflicts with Claude Code's own runtime state

### 6. All Python components use stdlib only

`ticket.py`, `swarm.py`, and `monitor/server.py` use only Python 3 standard library modules (sqlite3, argparse, json, http.server, etc.). No pip dependencies. The one exception is `interview-mcp.py` which requires the `mcp` library.

### 7. Dockerfile build context

The agent `Dockerfile` expects the build context to be `.swarm/` (not `agent/`), because it copies files from both `agent/` and `ticket/` subdirectories:
```dockerfile
COPY ticket/ticket.py /usr/local/lib/ticket.py
COPY agent/agent-loop.sh /usr/local/bin/agent-loop.sh
COPY agent/entrypoint.sh /usr/local/bin/entrypoint.sh
```

In `generate_docker_compose()`, the build context is `.swarm` with `dockerfile: agent/Dockerfile`.

## Ticket System

### Database

SQLite in WAL mode. Schema is managed via migrations in `ticket/migrations/`. Current schema version: 2.

```sql
schema_version (version, applied_at)        -- tracks migration state
tickets    (id, title, description, status, type, assigned_to, parent_id, created_by, created_at, updated_at)
blockers   (ticket_id, blocked_by)          -- composite PK
comments   (id, ticket_id, author, body, created_at)
activity_log (id, ticket_id, agent_id, action, detail, created_at)
```

Indexes on: `tickets.status`, `tickets.assigned_to`, `tickets.parent_id`, `comments.ticket_id`, `activity_log.ticket_id`.

### Migrations

Database schema changes are managed via SQL migration files in `ticket/migrations/`. Each file is numbered (e.g., `001_initial_schema.sql`, `002_add_ticket_type.sql`).

- `swarm init` runs migrations when creating a new database
- `swarm start` runs pending migrations before spinning up containers
- `ticket migrate` can be run manually to apply pending migrations

Every `ticket` command checks the schema version and errors if the database needs migration.

### Status values

| Status        | Meaning |
|---------------|---------|
| `open`        | Available for any agent to claim (unless it has open blockers) |
| `in_progress` | Claimed and actively being worked on |
| `ready`       | Agent signaled work complete; awaiting git push finalization |
| `done`        | Completed (set by `mark-done` after successful push, or by human override) |

There is **no `blocked` status**. Whether a ticket is blocked is derived from the `blockers` table at query time — if a ticket has entries pointing to non-`done` tickets, it is blocked. The `claim-next` command skips these automatically.

### Ticket types

| Type       | Purpose | Default when |
|------------|---------|--------------|
| `task`     | Normal work item for agents | Default for all tickets |
| `proposal` | Agent suggestion awaiting human approval | `--assign human` without blockers |
| `question` | Agent needs human input before proceeding | `--assign human` with `--blocked-by` |

Types enable the dashboard to show appropriate actions:
- **task**: Standard claim/complete workflow
- **proposal**: Approve / Approve with Edits / Reject buttons
- **question**: Answer & Unblock (posts answer to blocked ticket, marks question done)

Smart defaults: when creating a ticket assigned to `human`, the type defaults to `proposal` if standalone or `question` if it blocks another ticket. Use `--type` to override.

### CLI commands

```bash
ticket create "Title" [--description TEXT] [--parent ID] [--assign WHO]
                       [--blocked-by ID] [--created-by WHO] [--type task|proposal|question]
ticket update ID [--title TEXT] [--description TEXT] [--assign WHO] [--status STATUS]
                 [--type task|proposal|question]
ticket list [--status STATUS] [--assigned-to WHO] [--format text|json]
ticket show ID [--format text|json]
ticket count [--status STATUS]             # STATUS can be comma-separated: open,in_progress
ticket claim-next --agent AGENT [--format text|json]
ticket comment ID "BODY" [--author WHO]
ticket comments ID [--format text|json]
ticket complete ID
ticket mark-done ID                        # (hidden) finalize ticket after git push
ticket unclaim ID
ticket block ID --by ID
ticket unblock ID --by ID
ticket log [--limit N]
ticket migrate                             # Run pending database migrations
```

Database is found by: `--db` flag > `TICKET_DB` env var > walking up from cwd looking for `.swarm/tickets/tickets.db` > `./tickets.db`.

### claim-next logic

Atomic via `BEGIN IMMEDIATE`. Finds the oldest open ticket with no unresolved blockers and no assignee, claims it in a single transaction. Returns exit code 1 if no ticket is available.

### Blocker resolution

No automatic status changes when a blocker is resolved. Only `done` status unblocks dependents — `ready` does not. When a ticket is marked `done` (via `mark-done` or human override), any tickets it was blocking become naturally unblocked — the next `claim-next` will find them because their blockers are all `done`.

## Agent Container

### Image

Base: `node:lts-slim`. Installs: git, jq, curl, python3, cron, procps, Claude Code CLI (`npm install -g @anthropic-ai/claude-code`). Copies: `ticket.py`, `agent-loop.sh`, `entrypoint.sh`, `check-alive.sh`. Sets up crontab for stall detection.

### Volumes

| Container path | Source | Mode | Purpose |
|----------------|--------|------|---------|
| `/tickets` | `.swarm/tickets` bind mount | rw | SQLite database directory |
| `/repo.git` | `.swarm/repo.git` bind mount | rw | Bare git repo shared between agents |
| `/host-claude-config` | Host `~/.claude/` | ro | User preferences (NOT OAuth tokens) |
| `/workspace` | Container-local | rw | Agent's private git clone |

### Environment variables

| Variable | Set by | Purpose |
|----------|--------|---------|
| `AGENT_ID` | docker-compose.yml | Unique identifier (e.g. `agent-1`) |
| `CLAUDE_CODE_OAUTH_TOKEN` | `swarm start` (from Keychain) | OAuth access token for Claude API |
| `NTFY_TOPIC` | config.json | ntfy.sh topic for notifications (optional) |
| `MAX_TURNS` | config.json | Max Claude Code turns per ticket (default 50) |
| `ALLOWED_TOOLS` | config.json | Tools Claude can use (default `Bash,Read,Write,Edit,Glob,Grep`) |
| `TICKET_DB` | entrypoint.sh | Path to SQLite database (`/tickets/tickets.db`) |

### Startup sequence (entrypoint.sh)

1. Validates `CLAUDE_CODE_OAUTH_TOKEN` is set (fatal if missing)
2. Validates `AGENT_ID` is set
3. Clones from `/repo.git` into `/workspace` (or fetches if already exists)
4. Configures git identity as the agent ID (`user.name=$AGENT_ID`, `user.email=agent@swarm.local`)
5. Exports `TICKET_DB=/tickets/tickets.db`
6. Starts cron daemon for stall detection (runs `check-alive.sh` every 5 minutes)
7. Execs `agent-loop.sh`

### Agent loop (agent-loop.sh)

Infinite loop:

1. `ticket claim-next --agent $AGENT_ID` — atomically claim next available ticket
2. **If no ticket available:**
   - If queue is empty (no open or in_progress tickets): run Claude in read-only mode (`Read,Glob,Grep` only) to propose an improvement, assign proposal to human, sleep 5 min
   - If tickets exist but none claimable: sleep 10 sec
3. **If ticket claimed:**
   - Pull latest main, create branch `ticket-<id>`
   - Run `claude -p` with ticket details + instructions, allowed tools, max turns
   - Log Claude session output to `/workspace/.agent-logs/`
   - After Claude finishes: `git add -A`, commit as `ticket-<id>: <title>`
   - Push branch with rebase-and-retry on rejection (up to 3 attempts)
   - If merge conflict during rebase: invoke Claude for semantic conflict resolution
   - Merge branch into main, push main
   - Mark ticket `done` (via `mark-done`) on successful push, `unclaim` on failure
   - If no code changes: mark `done` without git operations

### Key design decisions

- **Git is handled by bash, not Claude.** The agent writes code; the script handles branching, committing, pushing, rebasing, and merging. Claude is only invoked for merge conflict resolution.
- **Rebase-and-retry on push rejection.** If another agent merged while this agent was working, the script pulls, rebases, and retries up to 3 times.
- **Unclaiming on failure.** If the agent gets stuck or a merge fails, the ticket is unclaimed and returns to the pool.
- **Parent context injection.** If a ticket has a parent, the parent's details are included in the Claude prompt.

### Crash detection and recovery

Three layers of protection prevent tickets from getting stuck when agents die unexpectedly:

1. **Unclaim all on swarm start** — `swarm start` automatically resets any `in_progress` tickets to `open` status before starting containers. This handles container crashes, OOM kills, scale-downs, and OAuth token expiry. Logged as "Auto-released on swarm start" in activity_log.

2. **SIGTERM trap in agent-loop.sh** — When Docker sends SIGTERM (via `docker stop`), the agent gracefully unclaims its current ticket, posts a comment, and exits cleanly. The ticket returns to the pool immediately.

3. **Cron-based stall detection** — Each container runs `check-alive.sh` every 5 minutes. If no log file activity for 20+ minutes, it sends SIGTERM to agent-loop, triggering graceful shutdown. Docker's `restart: unless-stopped` policy brings the container back fresh.

| Environment variable | Default | Purpose |
|---------------------|---------|---------|
| `STALE_THRESHOLD_MINUTES` | 20 | Minutes of inactivity before check-alive.sh restarts the agent |

Logs from stall detection are at `/var/log/check-alive.log` inside containers.

## Swarm CLI (`swarm/swarm.py`)

Installed at `~/.local/bin/swarm`. Finds the project root by walking up from cwd looking for `.swarm/`.

| Command | What it does |
|---------|--------------|
| `swarm init <dir>` | Scaffold `.swarm/`, generate configs, run Phase 2 interview |
| `swarm start` | Unclaim stuck tickets, extract OAuth token, `docker compose up -d --build` |
| `swarm stop` | `docker compose down` |
| `swarm status` | Show containers + ticket queue counts |
| `swarm logs <svc>` | `docker compose logs -f <service>` |
| `swarm scale N` | Update config, regenerate compose, restart |
| `swarm regenerate` | Re-copy source files into `.swarm/`, regenerate docker-compose.yml |
| `swarm pull` | Pull latest agent changes from the bare repo into host working tree |
| `swarm watch` | Poll bare repo and auto-pull new commits (default: every 5s) |

### `swarm init` phases

**Phase 1** (automatic): Creates `.swarm/` directory structure, copies agent/ticket/monitor files from the swarm toolkit repo, initializes SQLite database with schema, creates bare git repo from project's current state, generates `docker-compose.yml` via `generate_docker_compose()`, writes `CLAUDE.md` and `PROJECT.md` templates, creates a seed ticket.

**Phase 2** (interactive): Launches Claude Code with a system prompt to interview the human about business context (what the product does, who uses it, constraints, success criteria). Uses an MCP server (`interview-mcp.py`) that provides an `end_interview` tool. When Claude calls that tool, it writes a sentinel file. The swarm CLI polls for that file and sends SIGINT to end the session. The interview populates `PROJECT.md`. Technical decisions (database, framework, architecture) are deliberately excluded — agents make those.

## Monitor Web App (`monitor/`)

Python HTTP server (stdlib only) reading the SQLite database and Docker socket. Serves a self-contained SPA (vanilla HTML/CSS/JS, dark theme) on port 3000.

### REST API

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/api/tickets` | GET | List tickets (filterable by status, assigned_to) |
| `/api/tickets` | POST | Create a ticket |
| `/api/tickets/:id` | GET | Ticket detail with comments and blockers |
| `/api/tickets/:id/comment` | POST | Add a comment |
| `/api/tickets/:id/complete` | POST | Mark ticket done |
| `/api/tickets/:id/update` | POST | Update ticket fields |
| `/api/activity` | GET | Activity feed (chronological) |
| `/api/agents` | GET | Docker container status (via Docker socket) |
| `/api/agents/:name/logs` | GET | Container logs |
| `/api/stats` | GET | Summary counts |

### Dashboard tabs

- **Tickets**: Queue (open), In Progress, Blocked, Needs Human, Done
- **Agents**: Container status, CPU/memory, log viewer
- **Activity**: Chronological event feed

### Notifications

When a ticket is created with `assigned_to = 'human'`, a push notification is sent via ntfy.sh (if `NTFY_TOPIC` is configured).

## Human Interaction Patterns

- **Creating work**: Via `ticket` CLI or monitor web UI
- **Responding to agents**: Agent creates a ticket assigned to `human` that blocks the original ticket. Human gets notified, comments via CLI or web UI, marks the question ticket done. The blocked ticket automatically becomes available.
- **Monitoring**: Dashboard shows all ticket state, agent status, activity feed, container logs
- **Intervening**: Comment on in-progress tickets, reassign tickets, create blocking tickets to pause work, stop containers via Docker

## Working Conventions

- Use sub-agents liberally for exploration, planning, and implementation
- When spinning up a sub-agent, begin the prompt with "YOU ARE THE SUB-AGENT" so it does not try to launch its own sub-agents
- If your prompt does not begin with "YOU ARE THE SUB-AGENT", you are the orchestrator — your job is to coordinate sub-agents, not to write code directly
- Keep context windows short: supply all relevant context, no irrelevant context
- Every directory gets a README.md explaining its purpose and contents
- Update READMEs when code changes

## Keeping documentation in sync

Whenever you make a change that would impact either a README.md file or this CLAUDE.md, be sure to update the documentation so that it stays in sync. This way future runs benefit from the correct initial context.

## Archive

Previous approach (prescribed workflow state machine, Fastify/Drizzle/Postgres task API, Vue/Nuxt UI) is preserved in `archive/v1-workflow/` for reference.
